{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import time\n",
    "cd = 'C:\\\\Users\\\\Giada\\\\Documents\\\\GitHub\\\\Tweet-Sentiment-Predictor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a list of categories for each word in the vocab to be categorised into depending on its similarity to other words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If two words have a similarity of more than the limit then this function removes the less frequent word from the list\n",
    "def similar_word_remover(model,word_list,similarity_limit):\n",
    "    new_list = []\n",
    "    for i in range(len(word_list)):\n",
    "        for j in range(len(word_list)):\n",
    "            if model.wv.similarity(word_list[i],word_list[j]) > similarity_limit and model.wv.similarity(word_list[i],word_list[j]) != 1:\n",
    "                if i>j:\n",
    "                    new_list.append(word_list[i])\n",
    "                else:\n",
    "                    new_list.append(word_list[j])\n",
    "    new_list = list(dict.fromkeys(new_list))\n",
    "\n",
    "    final_list = word_list\n",
    "    for word in new_list:\n",
    "        final_list.remove(word)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a similarity matrix in a dataframe and output it to a csv file\n",
    "def similarity_matrix(model, word_list):\n",
    "    df = pd.DataFrame(columns = word_list, index=word_list)\n",
    "    for i in range(len(word_list)):\n",
    "        for j in range(len(word_list)):\n",
    "            df.iloc[i,j] = w2vmodel.wv.similarity(word_list[i],word_list[j])\n",
    "    df.to_csv('similarity_matrix_data.csv',index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many of the most frequent words do you want the categories to be chosen from?\n",
    "number_of_catagories = 250\n",
    "#Upper limit for word similarity\n",
    "similarity_limit = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel = gensim.models.KeyedVectors.load('w2vmodel')\n",
    "\n",
    "#Loading all the values form the word2vec into a dictionary and sorting them based on their frequency\n",
    "w2c = dict()\n",
    "for item in w2vmodel.wv.vocab:\n",
    "    w2c[item]=w2vmodel.wv.vocab[item].count\n",
    "w2cSorted=dict(sorted(w2c.items(), key = lambda x : x[1],reverse=True))\n",
    "\n",
    "#Creating a list of the most frequent words\n",
    "dict_words = w2cSorted.keys()\n",
    "ordered_words = list(dict_words)\n",
    "most_frequent_words = ordered_words[0:number_of_catagories]\n",
    "\n",
    "new_word_list = similar_word_remover(model=w2vmodel, word_list=most_frequent_words,similarity_limit=similarity_limit)\n",
    "similarity_matrix(model=w2vmodel, word_list=new_word_list)\n",
    "\n",
    "#Writing the final dataset categories to a txt file\n",
    "with open('Dataset_Categories.txt','w') as file:\n",
    "    for word in new_word_list:\n",
    "        file.write(word + \"\\n\")\n",
    "    file.write(\"OTHER\" + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running through the tweets and creating a dataset of frequency for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loops each word through the list of categories and assigns the word the most similar category\n",
    "def category_chooser(word,categories,w2vmodel):\n",
    "    most_similar_category = len(categories)-1\n",
    "    #best similarity chosen as a minimum cosine similarity needed for another category to be chosen\n",
    "    best_similarity = 0.6\n",
    "    for i in range(len(categories)-1):\n",
    "        temp_similarity = w2vmodel.wv.similarity(word,categories[i])\n",
    "        if temp_similarity > best_similarity: \n",
    "            best_similarity = temp_similarity\n",
    "            most_similar_category = i\n",
    "    return most_similar_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE RUNS THROUGH EACH WORD OF EACH TWEET AND ASSIGNS IT TO A CATEGORY\n",
    "#THE CODE TAKES A LONG TIME TO RUN: ~45mins\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Start time: \" % (start_time))\n",
    "#Reading a long list of categories from the dataset catagory file\n",
    "with open('Dataset_Categories.txt','r') as file:\n",
    "    categories = file.read().splitlines()\n",
    "file.close()\n",
    "\n",
    "df = pd.read_csv('tweets_Combined.csv', low_memory=False)\n",
    "SCORE = df.pop(\"SCORE\")\n",
    "dataset_final = pd.DataFrame(columns=categories,index=range(len(df)))\n",
    "w2vmodel = gensim.models.KeyedVectors.load('w2vmodel')\n",
    "\n",
    "#Loop to go through each of the tweets\n",
    "for tweet_number in range(len(df)): \n",
    "\n",
    "    #Initial setting of all categories to 0 \n",
    "    dataset_final.iloc[tweet_number,:] = 0\n",
    "\n",
    "    #Loop through all of the words in the tweet\n",
    "    for word_number in range(50):\n",
    "        word = df.iloc[tweet_number,word_number]\n",
    "        #If statement to check the words are in the model vocab\n",
    "        if word is not np.nan and word in w2vmodel.wv.vocab.keys():\n",
    "            category_number = category_chooser(word=word,categories=categories,w2vmodel=w2vmodel)\n",
    "            dataset_final.iloc[tweet_number,category_number] = dataset_final.iloc[tweet_number,category_number] + 1\n",
    "\n",
    "print(\"End time: \" % (time.time()))\n",
    "print(\"Time taken: \" % (time.time() - start_time))\n",
    "dataset_final[\"SCORE\"] = SCORE\n",
    "\n",
    "dataset_final.to_csv('modelling_dset.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
